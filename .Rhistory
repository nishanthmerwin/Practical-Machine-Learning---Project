plot(redid(fit),predict(fit))
plot(y=resid(fit),x=predict(fit))
plot(y=resid(fit),x=predict(fit),col=clean$classe)
plot(y=resid(fit),x=predict(fit),colour=clean$classe)
qplot(y=resid(fit),x=predict(fit),colour=clean$classe)
qplot(aes(y=resid(fit),x=predict(fit)),colour=clean$classe)
length(clean$classe)
length(resid(fit))
length(clean$classe)/5
resid(fit)
fit$residuals
fit
str(fit)
qplot(aes(y=resid(fit),x=predict(fit)),colour=clean$classe)
qplot(aes(y=resid(fit),x=predict(fit)))
df <- data.frame(x=predict(fit),y=resid(fit))
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point()
g
df <- data.frame(x=predict(fit),y=resid(fit))
head(df)
resid(fit)
fit<- lm(classe~.,data=clean)
head(resid(fit))
df <- data.frame(x=predict(fit),y=as.numeric(resid(fit)))
head(df)
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point()
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=19) + xlab("Linear Model Prediction") ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=19) + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=2,alpha=0.2) + xlab("Linear Model Prediction") + ylab("Residual")
g
head(resid(fit))
head(as.numeric(resid(fit)))
head(as.numeric(resid(fit)),n=20)
df <- data.frame(x=predict(fit),y=round(as.numeric(resid(fit)),n=0)
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=2,alpha=0.2) + xlab("Linear Model Prediction") + ylab("Residual")
g
head(df)
df <- data.frame(x=predict(fit),y=round(as.numeric(resid(fit)),n=0))
df <- data.frame(x=predict(fit),y=round(as.numeric(resid(fit)),n=1))
?round
df <- data.frame(x=round(predict(fit),digits=0),y=round(as.numeric(resid(fit)),digits=0))
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=2,alpha=0.2) + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(aes(size=count),pch=2,alpha=0.2) + xlab("Linear Model Prediction") + ylab("Residual")
g
?aggregate
df2 <- aggregate(df$x,by=list(x=df$x,y=df$y),length)
names(df2)[3] <- "count"
head(df2)
df2 <- aggregate(df$x,by=list(x=df$x,y=df$y),length)
names(df2)[3] <- "count"
g <- ggplot(data=df2,aes(x=x,y=y))
g <- g + geom_point(aes(size=count)) + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df2,aes(x=x,y=y))
g <- g + geom_point(aes(size=count*3)) + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df2,aes(x=x,y=y))
g <- g + geom_point(aes(size=count)) + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df2,aes(x=x,y=y))
g <- g + geom_point(aes(size=count)) + xlab("Linear Model Prediction") + ylab("Residual") + geom_point(colour="pink", size = 4)
g
g <- ggplot(data=df2,aes(x=x,y=y))
g <- g + geom_point(aes(size=count)) + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df2,aes(x=x,y=y))
g <- g + geom_point(aes(size=count),colour=count) + xlab("Linear Model Prediction") + ylab("Residual")
g
count
df2$count
g <- ggplot(data=df2,aes(x=x,y=y))
g <- g + geom_point(aes(size=count),colour=df2$count) + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df2,aes(x=x,y=y))
g <- g + geom_point(aes(size=count,colour=count)) + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df2,aes(x=x,y=y))
g <- g + geom_point(aes(size=count,colour=count)) + xlab("Linear Model Prediction") + ylab("Residual")
g <- scale_size_continuous(range = c(3, 7)
g
g <- scale_size_continuous(range = c(3, 7)
)
g
?scale_size_continuous
count
df2$count
g <- ggplot(data=df2,aes(x=x,y=y))
g <- g + geom_point(aes(size=count,colour=count)) + xlab("Linear Model Prediction") + ylab("Residual")
g <- scale_size_discrete(range = c(3, 7))
g
g <- ggplot(data=df2,aes(x=x,y=y))
g <- g + geom_point(aes(size=count,colour=count)) + xlab("Linear Model Prediction") + ylab("Residual")
g <- scale_size_discrete(range = c(3, 20))
g
?discrete_scale
library(caret)
?train
train(x=training, method="lm")
train(classe~.,data=training, method="lm")
train(classe~.,x=training, method="lm")
train(form= classe~.,x=training, method="lm")
train(x=training,y=training$classe method="lm")
train(x=training,y=training$classe, method="lm")
fit <- train(training[,-"classe"],training$classe,method="lm")
head(training[,-"classe"])
training[,"classe"]
training[-,"classe"]
training[,!"classe"]
training![,"classe"]
training[!,"classe"]
training[,"classe"!]
lm(classe~.,data=training)
fit <-lm(classe~.,data=training)
dim(training)
str(training)
df <- data.frame(x=predict(fit),y=as.numeric(resid(fit)))
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point() + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(aes(pch=1)) + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(aes(pch=19)) + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=19) + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=20) + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=1) + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=1,size=10) + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=1,size=10,alpha=0.2) + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=1,size=10,alpha=0.2,fill="grey") + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=1,size=10,alpha=0.2,fill="white") + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=1,size=20,alpha=0.2,fill="white") + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=10,size=20,alpha=0.2,fill="white") + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=10,size=21,alpha=0.2,bg="blue") + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=21,size=21,alpha=0.2,bg="blue") + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=21,size=20,alpha=0.2,bg="gray69") + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=21,size=20,alpha=0.2,bg="cyan") + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=21,size=20,alpha=0.2,bg="gray69",col="gray69") + xlab("Linear Model Prediction") + ylab("Residual")
g
fit<- lm(classe~.,data=clean)
qplot(aes(y=resid(fit),x=predict(fit)))
df <- data.frame(x=predict(fit),y=as.numeric(resid(fit)))
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=21,size=20,alpha=0.2,bg="gray69",col="gray69") + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=21,size=40,alpha=0.2,bg="gray69",col="gray69") + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=21,size=40,alpha=0.01,bg="gray69",col="gray69") + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=21,size=40,alpha=0.1,bg="gray69",col="gray69") + xlab("Linear Model Prediction") + ylab("Residual")
g
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(pch=21,size=40,alpha=0.1,bg="gray69",col="gray69") + xlab("Linear Model Prediction") + ylab("Residual")
g <- g + stat_smooth(method="lm")
g
?confusionMatrix
?predict
clean[,-classe]
clean[,-"classe"]
confusionMatrix(predict(fit,clean),clean$classe)
dim(clean)
test <- predict(clean[,-94],clean[,94])
test <- predict(fit,clean[,-94])
str(test)
confusionMatrix(clean$classe,test)
head(test)
inTrain <- createDataPartition(y=wholedata$classe,
p=0.7, list=FALSE)
wholedata <- read.csv("pml-training.csv",header=T)
inTrain <- createDataPartition(y=wholedata$classe,
p=0.7, list=FALSE)
training <- wholedata[inTrain,]; testing <- wholedata[-inTrain,]
dim(training); dim(testing)
remove <- nearzerovar(training)
remove <- nearZeroVar(training)
remove
names(training)
remove <- nearZeroVar(training)
remove <- c(remove,1,2,3,4,5,7)
training <- training[,-remove]
remove <- c(remove,1,2,3,4,5,7)
training <- training[,-remove]
dim(training)
names(training)
imp <- preProcess(training[,-60],method="knnImpute")
trainimp <- predict(imp,training[,-60])
install.packages("RANN")
library(RANN)
trainimp <- predict(imp,training[,-60])
names(trainimp)
trainimp <- cbind(trainimp,trainin$classe)
trainimp <- cbind(trainimp,training$classe)
dim(trainimp)
modFit  <- train(classe ~.,data=trainimp,method="lm")
names(trainimp)
modFit  <- train(training$classe ~.,data=trainimp,method="lm")
library(doParallel)
registerDoParallel(cores=2)
library(doParallel)
registerDoParallel(cores=4)
imp <- preProcess(training[,-60],method="knnImpute")
trainimp <- predict(imp,training[,-60])
modFit  <- train(training$classe ~.,data=trainimp,method="lm")
class(trainimp$training$classe)
trainimp <- cbind(trainimp,classe=training$classe)
names(trainimp)
class(trainimp$classe)
trainimp$classe
modFit  <- train(classe ~.,data=trainimp,method="lm")
modFit  <- train(classe ~.,data=trainimp,method="rpart")
str(modFit)
modFit$finalModel
intest <- predict(modFit,newdata=trainimp)
length(intest)
dim(trainimp)
confusionMatrix(intest,trainimp$classe)
set.seed(12345)
inTrain <- createDataPartition(y=wholedata$classe,
p=0.7, list=FALSE)
training <- wholedata[inTrain,]; testing <- wholedata[-inTrain,]
dim(training); dim(testing)
## Removing unneccesary features
remove <- nearZeroVar(training)
remove <- c(remove,1,2,3,4,5,7)
training <- training[,-remove]
## fill in missing values
imp <- preProcess(training[,-60],method="knnImpute")
trainimp <- predict(imp,training[,-60])
trainimp <- cbind(trainimp,classe=training$classe)
imp <- preProcess(training[,-60],method="knnImpute")
str(training)
training[] <- lapply(training, as.numeric)
str(training)
imp <- preProcess(training[,-60],method="knnImpute")
trainimp <- predict(imp,training[,-60])
modFit  <- train(classe ~.,data=trainimp,method="rf",prox=T)
library("randomForest")
library("caret")
names(getModelInfo())
dim(training)
## Testing to see if it works with a smaller size
set.seed(12345)
inTrain <- createDataPartition(y=trainimp$classe,
p=0.1, list=FALSE)
rftest <- trainimp[inTrain,]
dim(rftest)
names(rftest)
imp <- preProcess(training[,-99],method="knnImpute")
trainimp <- predict(imp,training[,-99])
trainimp <- cbind(trainimp,classe=training$classe)
set.seed(12345)
inTrain <- createDataPartition(y=wholedata$classe,
p=0.7, list=FALSE)
training <- wholedata[inTrain,]; testing <- wholedata[-inTrain,]
dim(training); dim(testing)
remove <- nearZeroVar(training)
remove <- c(remove,1,2,3,4,5,7)
remove
training <- training[,-remove]
training[] <- lapply(training, as.numeric)
imp <- preProcess(training[,-100],method="knnImpute")
trainimp <- predict(imp,training[,-100])
trainimp <- cbind(trainimp,classe=training$classe)
dim(trainimp)
names(trainimp)
set.seed(12345)
inTrain <- createDataPartition(y=trainimp$classe,
p=0.05, list=FALSE)
rftest <- trainimp[inTrain,]
dim(rftest)
modFit<- train(classe~.,rftest,method="parRF",trControl=trainControl(method="cv",number=5),
prox=TRUE,allowParallel=TRUE)
dim(rftest)
names(rftest)
modFit<- train(classe~.,rftest,method="gbm",verbose=F)
confusionMatrix(intest,trainimp$classe)
set.seed(12345)
inTrain <- createDataPartition(y=wholedata$classe,
p=0.7, list=FALSE)
training <- wholedata[inTrain,]; testing <- wholedata[-inTrain,]
dim(training); dim(testing)
remove <- nearZeroVar(training)
remove <- c(remove,1,2,3,4,5,7)
remove
training <- training[,-remove]
imp <- preProcess(training[,-100],method="knnImpute")
trainimp <- predict(imp,training[,-100])
trainimp <- cbind(trainimp,classe=training$classe)
modFit  <- train(classe ~.,data=trainimp,method="rpart")
modFit  <- train(classe ~.,data=trainimp,method="rpart")
library(doParallel)
registerDoParallel(cores=1)
modFit  <- train(classe ~.,data=trainimp,method="rpart")
intest <- predict(modFit,newdata=trainimp)
confusionMatrix(intest,trainimp$classe)
?cor
cor(wholedata)
cor(classe~.,training)
cor(classe~.,data=training)
remove <- nearZeroVar(training,saveMetrics=T)
remove
class(remove)
rownames(remove)
training[,rownames(remove)]
dim(training[,rownames(remove)])
confusionMatrix(intest,trainimp$classe)
c("roll_belt","pitch_forearm","yaw_belt","magnet_dumbbell_y","pitch_belt","magnet_dumbbell_z","roll_forearm",
"accel_dumbbell_y","roll_dumbbell","magnet_dumbbell_x","accel_forearm_x","magnet_belt_z","total_accel_dumbbell",
"magnet_forearm_z","accel_dumbbell_z","magnet_belt_y","accel_belt_z","yaw_arm","gyros_belt_z","magnet_belt_x")
features <- c("roll_belt","pitch_forearm","yaw_belt","magnet_dumbbell_y","pitch_belt","magnet_dumbbell_z","roll_forearm",
"accel_dumbbell_y","roll_dumbbell","magnet_dumbbell_x","accel_forearm_x","magnet_belt_z","total_accel_dumbbell",
"magnet_forearm_z","accel_dumbbell_z","magnet_belt_y","accel_belt_z","yaw_arm","gyros_belt_z","magnet_belt_x")
rawData <- read.csv("pml-training.csv", na.strings=c("NA",""), strip.white=T)
dim(rawData)
validData <- rawData[,features]
features <- c("roll_belt","pitch_forearm","yaw_belt","magnet_dumbbell_y","pitch_belt","magnet_dumbbell_z","roll_forearm",
"accel_dumbbell_y","roll_dumbbell","magnet_dumbbell_x","accel_forearm_x","magnet_belt_z","total_accel_dumbbell",
"magnet_forearm_z","accel_dumbbell_z","magnet_belt_y","accel_belt_z","yaw_arm","gyros_belt_z","magnet_belt_x")
validData <- rawData[,features]
inTrain <- createDataPartition(validData$classe, p=0.7, list=F)
training <- validData[inTrain,]
testing <- validData[-inTrain,]
features <- c("roll_belt","pitch_forearm","yaw_belt","magnet_dumbbell_y","pitch_belt","magnet_dumbbell_z","roll_forearm",
"accel_dumbbell_y","roll_dumbbell","magnet_dumbbell_x","accel_forearm_x","magnet_belt_z","total_accel_dumbbell",
"magnet_forearm_z","accel_dumbbell_z","magnet_belt_y","accel_belt_z","yaw_arm","gyros_belt_z","magnet_belt_x","classe")
validData <- rawData[,features]
inTrain <- createDataPartition(validData$classe, p=0.7, list=F)
training <- validData[inTrain,]
testing <- validData[-inTrain,]
ctrl <- trainControl(allowParallel=T, method="cv", number=4)
model <- train(classe ~ ., data=training, model="rf", trControl=ctrl)
pred <- predict(model, newdata=testing)
confusionMatrix(pred,testing$classe)
save(model,"RFModel.rda")
?save
save(model,file="RFModel.rda")
plot(model)
varImpPlot(model)
class(model)
confusionMatrix(pred,testing$classe)
?read.csv
?trainControl
#generate random data
data = data.frame(sample(LETTERS[0:20], 100, replace=T),sample(LETTERS[0:20], 100, replace=T))
names(data) = c("Actual", "Predicted")
#compute frequency of actual categories
actual = as.data.frame(table(data$Actual))
names(actual) = c("Actual","ActualFreq")
#build confusion matrix
confusion = as.data.frame(table(data$Actual, data$Predicted))
names(confusion) = c("Actual","Predicted","Freq")
#calculate percentage of test cases based on actual frequency
confusion = merge(confusion, actual, by=c(&quot;Actual&quot;))
confusion$Percent = confusion$Freq/confusion$ActualFreq*100
#render plot
# we use three different layers
# first we draw tiles and fill color based on percentage of test cases
tile <- ggplot() +
geom_tile(aes(x=Actual, y=Predicted,fill=Percent),data=confusion, color="black",size=0.1) +
labs(x="Actual",y="Predicted")
tile = tile +
geom_text(aes(x=Actual,y=Predicted, label=sprintf("%.1f", Percent)),data=confusion, size=3, colour="black") +
scale_fill_gradient(low="grey",high="red")
# lastly we draw diagonal tiles. We use alpha = 0 so as not to hide previous layers but use size=0.3 to highlight border
tile = tile +
geom_tile(aes(x=Actual,y=Predicted),data=subset(confusion, as.character(Actual)==as.character(Predicted)), color="black",size=0.3, fill="black", alpha=0)
#render
tile
tile <- ggplot() +
geom_tile(aes(x=Actual, y=Predicted,fill=Percent),data=confusion, color="black",size=0.1) +
labs(x="Actual",y="Predicted")
tile = tile +
geom_text(aes(x=Actual,y=Predicted, label=sprintf("%.1f", Percent)),data=confusion, size=3, colour="black") +
scale_fill_gradient(low="grey",high="red")
tile
confusion$Percent = confusion$Freq/confusion$ActualFreq*100
cm <-  confusionMatrix(pred,testing$classe)
str(cm)
cm$table
cm$table
class(cm$table)
class(cm)
confusionImage(cm)
install.packages("mlearning")
library(mlearning)
confusionImage(cm)
cm$confusion
str(cm)
?as.confusion
confusion(pred,testing$classe)
conf <- confusion(pred,testing$classe)
class(conf)
confusionImage(conf)
confusionDendogram(conf)
confusionDendrogram(conf)
confusionStars(conf)
confusionImage(conf)
dimnames(conf)
?confusionImage
confusionImage(conf,labels=names(dimnames(conf)),sort=F,numbers=F,ncols=11,names=c("Actual","Predicted"))
confusionImage(conf,labels=names(dimnames(conf)),sort=F,numbers=R,ncols=11,names=c("Actual","Predicted"))
confusionImage(conf,labels=names(dimnames(conf)),sort=F,numbers=T,ncols=11,names=c("Actual","Predicted"))
confusionImage(conf,labels=names(dimnames(conf)),sort=F,numbers=T,ncols=11,names=c("Actual","Predicted"),cex.axis=NULL)
confusionImage(conf,labels=names(dimnames(conf)),sort=F,numbers=T,ncols=11,names=c("Actual","Predicted"),cex.axis=NULL,
grid.col=NULL)
confusionImage(conf,sort=F,numbers=T,ncols=11,names=c("Actual","Predicted"),cex.axis=NULL,
grid.col=NULL)
warnings()
conf
str(conf)
conf$table
class(conf)
conf$Actual
cm
str(cm)
cm$overall[1]
knit2html("FinalReport.rmd")
library(knitr)
knit2html("FinalReport.rmd")
browseURL("FinalReport.html")
knit2html("FinalReport.rmd")
browseURL("FinalReport.html")
knit2html("FinalReport.rmd")
browseURL("FinalReport.html")
realtest <- read.csv("~/Coursera/MachineLearning/testing/pml-testing.csv")
realtest
dim(realtest)
str(realtest)
head(realtest[,160])
realtest[,160]
names(realtest)[160]
realtest <- realtest[,features]
features <- c("roll_belt","pitch_forearm","yaw_belt","magnet_dumbbell_y","pitch_belt","magnet_dumbbell_z","roll_forearm",
"accel_dumbbell_y","roll_dumbbell","magnet_dumbbell_x","accel_forearm_x","magnet_belt_z","total_accel_dumbbell",
"magnet_forearm_z","accel_dumbbell_z","magnet_belt_y","accel_belt_z","yaw_arm","gyros_belt_z","magnet_belt_x","classe")
warnings()
features <- c("roll_belt","pitch_forearm","yaw_belt","magnet_dumbbell_y","pitch_belt","magnet_dumbbell_z","roll_forearm",
"accel_dumbbell_y","roll_dumbbell","magnet_dumbbell_x","accel_forearm_x","magnet_belt_z","total_accel_dumbbell",
"magnet_forearm_z","accel_dumbbell_z","magnet_belt_y","accel_belt_z","yaw_arm","gyros_belt_z","magnet_belt_x","classe")
realtest <- realtest[,features]
dim(realtest)
realtest <- realtest[,features]
realtest[,features]
realtest <- realtest[,features]
realtest <- read.csv("~/Coursera/MachineLearning/testing/pml-testing.csv", na.strings=c("NA",""), strip.white=T)
features <- c("roll_belt","pitch_forearm","yaw_belt","magnet_dumbbell_y","pitch_belt","magnet_dumbbell_z","roll_forearm",
"accel_dumbbell_y","roll_dumbbell","magnet_dumbbell_x","accel_forearm_x","magnet_belt_z","total_accel_dumbbell",
"magnet_forearm_z","accel_dumbbell_z","magnet_belt_y","accel_belt_z","yaw_arm","gyros_belt_z","magnet_belt_x")
realtest <- realtest[,features]
dim(realtest)
pred <- predict(model, newdata=realtest)
pred
setwd("~/Coursera/MachineLearning/testing")
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(pred)
features
names(validData)
knit2html("FinalReport.rmd")
getwd()
setwd("~/Coursera/MachineLearning")
knit2html("FinalReport.rmd")
browseURL("FinalReport.html")
knit2html("FinalReport.rmd")
